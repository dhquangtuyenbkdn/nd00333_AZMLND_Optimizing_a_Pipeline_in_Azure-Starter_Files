# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
    This dataset contains data about user for using loan with same info same as age, job
    We seek to predict for hight rate loan user with age, job, have hounsing or not

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
    The bast model is StackEnsemble
## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
    1: Load train data, Clean and transform the data
        - Load train data from URL with TabularDataset
        - Clean and transform with clean_data function
    
    2: Regression Scikit-learn with version, config from conda_dependencies.yml
    
    3: Config HyperDriver with parameters:
        - Resource run with train.py by cluser_name, and Scikit-learn of (2)
        - Parameter sampler with RandomParameterSampling
        - BanditPolicy for early stopping polic
        - Accuracy primary metric with Maximize goal
        
        HyperDriveConfig(run_config=src,
                         hyperparameter_sampling=ps,
                         policy=policy,
                         primary_metric_name='accuracy',
                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,
                         max_total_runs=4,
                         max_concurrent_runs=4)
        
    4: HyperDriver submit to Experiment to Train model
    
    5: Research report with Best run model

**What are the benefits of the parameter sampler you chose?**
    Refer: https://github.com/abhiojha8/Optimizing_ML_Pipeline_Azure
    - RandomParameterSampling(
        {
            '--C' : choice(0.001,0.01,0.1,1,10,100),
            '--max_iter': choice(50,100,200)
        }
    )

**What are the benefits of the early stopping policy you chose?**
    - BanditPolicy(slack_factor = 0.1, evaluation_interval = 2, delay_evaluation = 5)

**I my runtime, hyperdrive_run.get_best_run_by_primary_metric() always return null, so i can print and record the best run in file module, result i will check in Child jobs**

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
    1: Load train data from dataset was load from url
    
    2 Clean and transform the data clean_data function from train.py
    
    3: Config AutoML with parameters:
        - Experiment timeout for each child jobs
        - AutoML run with classification algorithm with Accuracy primary metric
        - Dataset was load at (1) for train data model with forcus 'Loan' label
        - And 5 cross validations to perform when validation data is not specified.
        
        AutoMLConfig(   experiment_timeout_minutes=30,
                        task='classification',
                        compute_target = cluster,
                        primary_metric='accuracy',
                        training_data=dataset,
                        label_column_name='y',
                        n_cross_validations= 5)
        
    4: Submit AutoML to Experiment to train model
    
    5: Research report with Best run model
    
**I my runtime, environment has go to time out, so i can't run last report method**
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
    - HyperDriver:  0.916
    - AutoML (VotingEnsemble):  0.9181
    
    They have same performance but with AutoML Train, the time run to long then HyperDriver, it make Best run result is better
    In status report, the first half of the train time, AutoML may be same of no better than HyperDriver.
    
    ****
    
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
    - Increase timeout, longtime pipeline runing will be incress result

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
